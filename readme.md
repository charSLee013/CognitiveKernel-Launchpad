# ðŸ§  CognitiveKernel-Launchpad â€” Open Framework for Deep Research Agents & Agent Foundation Models

> ðŸŽ“ **Academic Research & Educational Use Only** â€” No Commercial Use
> ðŸ“„ [Paper (arXiv:2508.00414)](https://arxiv.org/abs/2508.00414) | ðŸ‡¨ðŸ‡³ [ä¸­æ–‡æ–‡æ¡£](README_zh.md) | ðŸ“œ [LICENSE](LICENSE.txt)

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![arXiv](https://img.shields.io/badge/arXiv-2508.00414-b31b1b.svg)](https://arxiv.org/abs/2508.00414)

---

## ðŸŒŸ Why CognitiveKernel-Launchpad?

This research-only fork is derived from Tencent's original CognitiveKernel-Pro and is purpose-built for inference-time usage. It removes complex training/SFT and heavy testing pipelines, focusing on a clean reasoning runtime that is easy to deploy for distributed inference. In addition, it includes a lightweight Gradio web UI for convenient usage.

---

## ðŸš€ Quick Start

### 1. Install (No GPU Required)

```bash
git clone https://github.com/charSLee013/CognitiveKernel-Launchpad.git
cd CognitiveKernel-Launchpad
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Set Environment (Minimal Setup)

```bash
export OPENAI_API_KEY="sk-..."
export OPENAI_API_BASE="https://api.openai.com/v1"
export OPENAI_API_MODEL="gpt-4o-mini"
```

### 3. Run a Single Question

```bash
python -m ck_pro "What is the capital of France?"
```

âœ… Thatâ€™s it! Youâ€™re running a deep research agent.

---

## ðŸ› ï¸ Core Features

### ðŸ–¥ï¸ CLI Interface
```bash
python -m ck_pro \
  --config config.toml \
  --input questions.txt \
  --output answers.txt \
  --interactive \
  --verbose
```

| Flag          | Description                          |
|---------------|--------------------------------------|
| `-c, --config`| TOML config path (optional)          |
| `-i, --input` | Batch input file (one Q per line)    |
| `-o, --output`| Output answers to file               |
| `--interactive`| Start interactive Q&A session       |
| `-v, --verbose`| Show reasoning steps & timing       |

---

### âš™ï¸ Configuration (config.toml)

> `TOML > Env Vars > Defaults`

Use the examples in this repo:
- Minimal config: [config.minimal.toml](config.minimal.toml) â€” details in [CONFIG_EXAMPLES.md](CONFIG_EXAMPLES.md)
- Comprehensive config: [config.comprehensive.toml](config.comprehensive.toml) â€” full explanation in [CONFIG_EXAMPLES.md](CONFIG_EXAMPLES.md)

#### ðŸš€ Recommended Configuration

Based on the current setup, here's the recommended configuration for optimal performance:

```toml
# Core Agent Configuration
[ck.model]
call_target = "https://api-inference.modelscope.cn/v1/chat/completions"
api_key = "your-modelscope-api-key-here"  # Replace with your actual key
model = "Qwen/Qwen3-235B-A22B-Instruct-2507"

[ck.model.extract_body]
temperature = 0.6
max_tokens = 8192

# Web Agent Configuration (for web browsing tasks)
[web]
max_steps = 20
use_multimodal = "auto"  # Automatically use multimodal when needed

[web.model]
call_target = "https://api-inference.modelscope.cn/v1/chat/completions"
api_key = "your-modelscope-api-key-here"  # Replace with your actual key
model = "moonshotai/Kimi-K2-Instruct"
request_timeout = 600
max_retry_times = 5
max_token_num = 8192

[web.model.extract_body]
temperature = 0.0
top_p = 0.95
max_tokens = 8192

# Multimodal Web Agent (for visual tasks)
[web.model_multimodal]
call_target = "https://api-inference.modelscope.cn/v1/chat/completions"
api_key = "your-modelscope-api-key-here"  # Replace with your actual key
model = "Qwen/Qwen2.5-VL-72B-Instruct"
request_timeout = 600
max_retry_times = 5
max_token_num = 8192

[web.model_multimodal.extract_body]
temperature = 0.0
top_p = 0.95
max_tokens = 8192

# Search Configuration
[search]
backend = "duckduckgo"  # Recommended: reliable and no API key required
```

#### ðŸ”‘ API Key Setup

1. **Get ModelScope API Key**: Visit [ModelScope](https://www.modelscope.cn/) to obtain your API key
2. **Replace placeholders**: Update all `your-modelscope-api-key-here` with your actual API key
3. **Alternative**: Use environment variables:
   ```bash
   export MODELSCOPE_API_KEY="your-actual-key"
   ```

#### ðŸ“‹ Model Selection Rationale

- **Main Agent**: `Qwen3-235B-A22B-Instruct-2507` - Latest high-performance reasoning model
- **Web Agent**: `Kimi-K2-Instruct` - Optimized for web interaction tasks
- **Multimodal**: `Qwen2.5-VL-72B-Instruct` - Advanced vision-language capabilities

For all other options, see [CONFIG_EXAMPLES.md](CONFIG_EXAMPLES.md).

---

### ðŸ“Š GAIA Benchmark Evaluation

Evaluate your agent on the GAIA benchmark:

```bash
python -m gaia.cli.simple_validate \
  --data gaia_val.jsonl \
  --level all \
  --count 10 \
  --output results.jsonl
```

â†’ Outputs detailed performance summary & per-task results.

---

### ðŸŒ Gradio Web UI

Launch a user-friendly web interface:

```bash
python -m ck_pro.gradio_app --host 0.0.0.0 --port 7860
```

â†’ Open `http://localhost:7860` in your browser.


Note: It is recommended to install Playwright browsers (or install them if you encounter related errors). On Linux you may also need to run playwright install-deps.

Note: It is recommended to install Playwright browsers (or install them if you encounter related errors): `python -m playwright install` (Linux may also require `python -m playwright install-deps`).

---

### ðŸ“‚ Logging

- Console: `INFO` level by default
- Session logs: `logs/ck_session_*.log`
- Configurable via `[logging]` section in TOML

---

## ðŸ§© Architecture Highlights

- **Modular Design**: Web, File, Code, Reasoning modules
- **Fallback Mechanism**: HTTP API â†’ Playwright browser automation
- **Reflection & Voting**: Novel test-time strategies for improved accuracy
- **Extensible**: Easy to plug in new models, tools, or datasets

---

## ðŸ“œ License & Attribution

This is a research-only fork of **Tencentâ€™s CognitiveKernel-Pro**.
ðŸ”— Original: https://github.com/Tencent/CognitiveKernel-Pro

> âš ï¸ **Strictly for academic research and educational purposes. Commercial use is prohibited.**
> See `LICENSE.txt` for full terms.
